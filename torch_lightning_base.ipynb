{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linhdb/miniconda3/envs/torch_boost/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSegmentationDataset(Dataset):\n",
    "    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, feature_extractor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
    "            feature_extractor (SegFormerFeatureExtractor): feature extractor to prepare images + segmentation maps.\n",
    "            train (bool): Whether to load \"training\" or \"validation\" images + annotations.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "        image_file_names = [f for f in os.listdir(self.root_dir) if '.jpg' in f]\n",
    "        mask_file_names = [f for f in os.listdir(self.root_dir.replace('images', 'masks')) if '_mask.png' in f]\n",
    "        \n",
    "        self.images = sorted(image_file_names)\n",
    "        self.masks = sorted(mask_file_names)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = Image.open(os.path.join(self.root_dir, self.images[idx]))\n",
    "        segmentation_map = Image.open(os.path.join(self.root_dir.replace('images', 'masks'), self.masks[idx]))\n",
    "\n",
    "        # randomly crop + pad both image and segmentation map to same size\n",
    "        encoded_inputs = self.feature_extractor(image, segmentation_map, return_tensors=\"pt\")\n",
    "\n",
    "        for k,v in encoded_inputs.items():\n",
    "          encoded_inputs[k].squeeze_() # remove batch dimension\n",
    "\n",
    "        return encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegformerFinetuner(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, train_dataloader=None, val_dataloader=None, test_dataloader=None, metrics_interval=100):\n",
    "        super(SegformerFinetuner, self).__init__()\n",
    "        self.metrics_interval = metrics_interval\n",
    "        self.train_dl = train_dataloader\n",
    "        self.val_dl = val_dataloader\n",
    "        self.test_dl = test_dataloader\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "        \n",
    "        self.num_classes = 2\n",
    "        \n",
    "        self.label2id = {0: 'background', 1: 'receipt'}\n",
    "        self.id2label = {v:k for k,v in self.label2id.items()}\n",
    "\n",
    "        self.model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "            \"nvidia/segformer-b0-finetuned-ade-512-512\", \n",
    "            return_dict=False, \n",
    "            num_labels=self.num_classes,\n",
    "            id2label=self.id2label,\n",
    "            label2id=self.label2id,\n",
    "            ignore_mismatched_sizes=True,\n",
    "        )\n",
    "        \n",
    "        self.train_mean_iou = load_metric(\"mean_iou\")\n",
    "        self.val_mean_iou = load_metric(\"mean_iou\")\n",
    "        self.test_mean_iou = load_metric(\"mean_iou\")\n",
    "        \n",
    "    def forward(self, images, masks):\n",
    "        outputs = self.model(pixel_values=images, labels=masks)\n",
    "        return(outputs)\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        \n",
    "        images, masks = batch['pixel_values'], batch['labels']\n",
    "        \n",
    "        outputs = self(images, masks)\n",
    "        \n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "        \n",
    "        upsampled_logits = nn.functional.interpolate(\n",
    "            logits, \n",
    "            size=masks.shape[-2:], \n",
    "            mode=\"bilinear\", \n",
    "            align_corners=False\n",
    "        )\n",
    "\n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "\n",
    "        self.train_mean_iou.add_batch(\n",
    "            predictions=predicted.detach().cpu().numpy(), \n",
    "            references=masks.detach().cpu().numpy()\n",
    "        )\n",
    "        if batch_nb % self.metrics_interval == 0:\n",
    "\n",
    "            metrics = self.train_mean_iou.compute(\n",
    "                num_labels=self.num_classes, \n",
    "                ignore_index=255, \n",
    "                reduce_labels=False,\n",
    "            )\n",
    "            \n",
    "            metrics = {'loss': loss, \"mean_iou\": metrics[\"mean_iou\"], \"mean_accuracy\": metrics[\"mean_accuracy\"]}\n",
    "            \n",
    "            for k,v in metrics.items():\n",
    "                self.log(k,v)\n",
    "            \n",
    "            return(metrics)\n",
    "        else:\n",
    "            return({'loss': loss})\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        \n",
    "        images, masks = batch['pixel_values'], batch['labels']\n",
    "        \n",
    "        outputs = self(images, masks)\n",
    "        \n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "        self.validation_step_outputs.append(loss)\n",
    "        \n",
    "        upsampled_logits = nn.functional.interpolate(\n",
    "            logits, \n",
    "            size=masks.shape[-2:], \n",
    "            mode=\"bilinear\", \n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "        \n",
    "        self.val_mean_iou.add_batch(\n",
    "            predictions=predicted.detach().cpu().numpy(), \n",
    "            references=masks.detach().cpu().numpy()\n",
    "        )\n",
    "        \n",
    "        return({'val_loss': loss})\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        metrics = self.val_mean_iou.compute(\n",
    "            num_labels=self.num_classes, \n",
    "            ignore_index=255, \n",
    "            reduce_labels=False,\n",
    "        )\n",
    "        avg_val_loss = torch.stack(self.validation_step_outputs).mean()\n",
    "        val_mean_iou = metrics[\"mean_iou\"]\n",
    "        val_mean_accuracy = metrics[\"mean_accuracy\"]\n",
    "        \n",
    "        metrics = {\"val_loss\": avg_val_loss, \"val_mean_iou\": val_mean_iou, \"val_mean_accuracy\": val_mean_accuracy}\n",
    "        for k,v in metrics.items():\n",
    "            self.log(k,v)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        \n",
    "        images, masks = batch['pixel_values'], batch['labels']\n",
    "        \n",
    "        outputs = self(images, masks)\n",
    "        \n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "\n",
    "        self.test_step_outputs.append(loss)\n",
    "        \n",
    "        upsampled_logits = nn.functional.interpolate(\n",
    "            logits, \n",
    "            size=masks.shape[-2:], \n",
    "            mode=\"bilinear\", \n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "        \n",
    "        self.test_mean_iou.add_batch(\n",
    "            predictions=predicted.detach().cpu().numpy(), \n",
    "            references=masks.detach().cpu().numpy()\n",
    "        )\n",
    "            \n",
    "        return({'test_loss': loss})\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        metrics = self.test_mean_iou.compute(\n",
    "              num_labels=self.num_classes, \n",
    "              ignore_index=255, \n",
    "              reduce_labels=False,\n",
    "          )\n",
    "       \n",
    "        avg_test_loss = torch.stack(self.test_step_outputs).mean()\n",
    "        test_mean_iou = metrics[\"mean_iou\"]\n",
    "        test_mean_accuracy = metrics[\"mean_accuracy\"]\n",
    "\n",
    "        metrics = {\"test_loss\": avg_test_loss, \"test_mean_iou\":test_mean_iou, \"test_mean_accuracy\":test_mean_accuracy}\n",
    "        \n",
    "        for k,v in metrics.items():\n",
    "            self.log(k,v)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=2e-05, eps=1e-08)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.train_dl\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self.val_dl\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return self.test_dl\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linhdb/miniconda3/envs/torch_boost/lib/python3.8/site-packages/transformers/models/segformer/image_processing_segformer.py:99: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([2, 256, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_25466/2370556508.py:26: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  self.train_mean_iou = load_metric(\"mean_iou\")\n"
     ]
    }
   ],
   "source": [
    "dataset_location = \"data/mcocr\"\n",
    "feature_extractor = SegformerImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "feature_extractor.do_reduce_labels = False\n",
    "feature_extractor.size = 128\n",
    "\n",
    "train_dataset = SemanticSegmentationDataset(f\"{dataset_location}/images/train/\", feature_extractor)\n",
    "val_dataset = SemanticSegmentationDataset(f\"{dataset_location}/images/val/\", feature_extractor)\n",
    "test_dataset = SemanticSegmentationDataset(f\"{dataset_location}/images/test/\", feature_extractor)\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 8\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "segformer_finetuner = SegformerFinetuner(\n",
    "    train_dataloader=train_dataloader, \n",
    "    val_dataloader=val_dataloader, \n",
    "    test_dataloader=test_dataloader, \n",
    "    metrics_interval=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\", \n",
    "    min_delta=0.00, \n",
    "    patience=10, \n",
    "    verbose=False, \n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(save_top_k=1, monitor=\"val_loss\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices = 1, \n",
    "    log_every_n_steps = 20,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    max_epochs=10,\n",
    "    val_check_interval=len(train_dataloader),\n",
    "    profiler=\"simple\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                             | Params\n",
      "-----------------------------------------------------------\n",
      "0 | model | SegformerForSemanticSegmentation | 3.7 M \n",
      "-----------------------------------------------------------\n",
      "3.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.7 M     Total params\n",
      "14.859    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [02:07<00:00,  3.46s/it, v_num=8]          Time elapsed 18.36 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linhdb/miniconda3/envs/torch_boost/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "trainer.fit(segformer_finetuner)\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(f\"Time elapsed {elapsed/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 epochs ~ 18-20mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = trainer.test(ckpt_path=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('/mnt/d/SUN/server/pytorch_train_enhance/data/Balloons-mask-semantic/test/15290896925_884ab33fd3_k_jpg.rf.35191853c8f633667636765fb640425d_mask.png').size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = {\n",
    "    0:(0,0,0),\n",
    "    1:(255,0,0),\n",
    "}\n",
    "\n",
    "def prediction_to_vis(prediction):\n",
    "    vis_shape = prediction.shape + (3,)\n",
    "    vis = np.zeros(vis_shape)\n",
    "    for i,c in color_map.items():\n",
    "        vis[prediction == i] = color_map[i]\n",
    "    return Image.fromarray(vis.astype(np.uint8))\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    images, masks = batch['pixel_values'], batch['labels']\n",
    "    outputs = segformer_finetuner.model(images, masks)\n",
    "        \n",
    "    loss, logits = outputs[0], outputs[1]\n",
    "\n",
    "    upsampled_logits = nn.functional.interpolate(\n",
    "        logits, \n",
    "        size=masks.shape[-2:], \n",
    "        mode=\"bilinear\", \n",
    "        align_corners=False\n",
    "    )\n",
    "    predicted_mask = upsampled_logits.argmax(dim=1).cpu().numpy()\n",
    "    masks = masks.cpu().numpy()\n",
    "\n",
    "n_plots = 4\n",
    "from matplotlib import pyplot as plt\n",
    "f, axarr = plt.subplots(n_plots,2)\n",
    "f.set_figheight(15)\n",
    "f.set_figwidth(15)\n",
    "for i in range(n_plots):\n",
    "    axarr[i,0].imshow(prediction_to_vis(predicted_mask[i,:,:]))\n",
    "    axarr[i,1].imshow(prediction_to_vis(masks[i,:,:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict on a test image and overlay the mask on the original image\n",
    "test_idx = 2\n",
    "input_image_file = os.path.join(test_dataset.root_dir, test_dataset.images[test_idx])\n",
    "input_image = Image.open(input_image_file)\n",
    "test_batch = test_dataset[test_idx]\n",
    "images, masks = test_batch['pixel_values'], test_batch['labels']\n",
    "images = torch.unsqueeze(images, 0)\n",
    "masks = torch.unsqueeze(masks, 0)\n",
    "outputs = segformer_finetuner.model(images, masks)\n",
    "    \n",
    "loss, logits = outputs[0], outputs[1]\n",
    "\n",
    "upsampled_logits = nn.functional.interpolate(\n",
    "    logits, \n",
    "    size=masks.shape[-2:], \n",
    "    mode=\"bilinear\", \n",
    "    align_corners=False\n",
    ")\n",
    "predicted_mask = upsampled_logits.argmax(dim=1).cpu().numpy()\n",
    "mask = prediction_to_vis(np.squeeze(masks))\n",
    "mask = mask.resize(input_image.size)\n",
    "mask = mask.convert(\"RGBA\")\n",
    "input_image = input_image.convert(\"RGBA\")\n",
    "overlay_img = Image.blend(input_image, mask, 0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_boost",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
